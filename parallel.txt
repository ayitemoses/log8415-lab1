Yes‚Äîlet‚Äôs make it parallel. The two slow bits are (1) waiting for EC2 to reach ‚Äúrunning‚Äù and (2) per-host apt/pip work. Below are **drop-in updates** that (a) wait for **all** instances at once and (b) deploy to hosts **concurrently** with a tunable worker count.

---

### 1) Faster provisioning wait (wait for all at once)

Replace your `scripts/provision_instances.py` with this version (only the wait/load part changed):

```python
#!/usr/bin/env python3
import json, os, sys, itertools
import boto3

REGION   = os.getenv("AWS_REGION", "us-east-1")
KEY_NAME = os.getenv("AWS_KEY_NAME")
SG_ID    = os.getenv("AWS_INSTANCE_SG_ID")
AMI_ID   = os.getenv("AWS_AMI_ID", "")
SUBNETS  = os.getenv("AWS_SUBNET_IDS", "").split(",") if os.getenv("AWS_SUBNET_IDS") else []

if not (KEY_NAME and SG_ID and SUBNETS and len(SUBNETS) >= 2):
    sys.exit("Missing one of: AWS_KEY_NAME, AWS_INSTANCE_SG_ID, AWS_SUBNET_IDS(2+)")

ec2r = boto3.resource("ec2", region_name=REGION)
ec2  = boto3.client("ec2", region_name=REGION)
ssm  = boto3.client("ssm", region_name=REGION)

if not AMI_ID:
    try:
        AMI_ID = ssm.get_parameter(
            Name="/aws/service/canonical/ubuntu/server/22.04/stable/current/amd64/hvm/ebs-gp3/ami-id"
        )["Parameter"]["Value"]
    except Exception:
        AMI_ID = ssm.get_parameter(
            Name="/aws/service/canonical/ubuntu/server/22.04/stable/current/amd64/hvm/ebs-gp2/ami-id"
        )["Parameter"]["Value"]

def create_group(instance_type: str, count: int, cluster_tag: str):
    rr = itertools.cycle(SUBNETS)
    instances = []
    for _ in range(count):
        subnet = next(rr)
        r = ec2r.create_instances(
            ImageId=AMI_ID,
            InstanceType=instance_type,
            MinCount=1, MaxCount=1,
            KeyName=KEY_NAME,
            NetworkInterfaces=[{
                "DeviceIndex": 0,
                "SubnetId": subnet,
                "AssociatePublicIpAddress": True,
                "Groups": [SG_ID],
            }],
            TagSpecifications=[{
                "ResourceType": "instance",
                "Tags": [
                    {"Key": "Name", "Value": f"lab-{cluster_tag}-{instance_type}"},
                    {"Key": "Cluster", "Value": cluster_tag},
                ],
            }],
        )
        instances.extend(r)
    return instances

print("Creating 4√ó t2.micro (cluster2) and 4√ó t2.large (cluster1)‚Ä¶")
grp_micro = create_group("t2.micro", 4, "cluster2")
grp_large = create_group("t2.large", 4, "cluster1")
all_instances = grp_micro + grp_large

# ---- Faster: wait for ALL to be running in one waiter call
ids = [i.id for i in all_instances]
ec2.get_waiter("instance_running").wait(InstanceIds=ids)

# Reload attributes in bulk
desc = ec2.describe_instances(InstanceIds=ids)
id_to_addrs = {}
for res in desc.get("Reservations", []):
    for inst in res.get("Instances", []):
        id_to_addrs[inst["InstanceId"]] = {
            "public_ip":  inst.get("PublicIpAddress"),
            "private_ip": inst.get("PrivateIpAddress"),
            "type":       inst.get("InstanceType"),
            "state":      inst.get("State", {}).get("Name", "unknown"),
            "tags":       inst.get("Tags", []),
        }

out = []
for i in all_instances:
    meta = id_to_addrs.get(i.id, {})
    cluster = next((t["Value"] for t in meta.get("tags", []) if t.get("Key")=="Cluster"), None)
    out.append({
        "id": i.id,
        "type": meta.get("type"),
        "state": meta.get("state"),
        "public_ip": meta.get("public_ip"),
        "private_ip": meta.get("private_ip"),
        "cluster": cluster,
    })

os.makedirs("artifacts", exist_ok=True)
with open("artifacts/instances.json", "w") as f:
    json.dump(out, f, indent=2)
print("‚úÖ Wrote artifacts/instances.json with instance IDs and IPs.")
```

---

### 2) Concurrent deploys over SSH/SCP

Replace your `scripts/deploy_fastapi.py` with this version (concurrency + tiny tweaks):

```python
#!/usr/bin/env python3
import json, os, sys, subprocess, pathlib, base64
import concurrent.futures as cf

KEY_PATH = os.getenv("AWS_KEY_PATH")
if not KEY_PATH:
    sys.exit("Missing AWS_KEY_PATH")

APP_SRC = pathlib.Path("app").resolve()
SSH_USER = "ubuntu"

# Tunable concurrency: DEPLOY_CONCURRENCY (default: min(8, cpu_count))
DEFAULT_WORKERS = min(8, (os.cpu_count() or 8))
MAX_WORKERS = int(os.getenv("DEPLOY_CONCURRENCY", str(DEFAULT_WORKERS)))
ONLY_CLUSTER = os.getenv("ONLY_CLUSTER")  # optional: "cluster1" or "cluster2"

SSH_BASE = [
    "ssh",
    "-o", "StrictHostKeyChecking=no",
    "-o", "BatchMode=yes",
    "-o", "ServerAliveInterval=15",
    "-o", "ServerAliveCountMax=3",
    "-o", "ConnectTimeout=20",
    "-o", "ConnectionAttempts=10",
]

def ssh(host, cmd):
    remote = f"bash -lc '{cmd}'"
    return subprocess.run(
        SSH_BASE + ["-i", KEY_PATH, f"{SSH_USER}@{host}", remote],
        stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True
    )

def scp_dir(host, local_path, remote_home="~"):
    return subprocess.run(
        ["scp", "-C", "-o", "StrictHostKeyChecking=no", "-i", KEY_PATH, "-r",
         local_path, f"{SSH_USER}@{host}:{remote_home}"],
        stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True
    )

with open("artifacts/instances.json") as f:
    instances = json.load(f)

SERVICE_PATH = "/etc/systemd/system/fastapi.service"

SERVICE_TPL = r"""[Unit]
Description=FastAPI (Uvicorn) service
After=network.target

[Service]
User=ubuntu
WorkingDirectory=/home/ubuntu/app
Environment=CLUSTER_NAME={cluster}
# Free the port if something old is around (requires psmisc)
ExecStartPre=/bin/bash -lc 'fuser -k 8000/tcp || true'
ExecStart=/usr/bin/python3 -m uvicorn main:app --host 0.0.0.0 --port 8000
Restart=always
RestartSec=2
TimeoutStartSec=30

# StandardOutput=append:/var/log/fastapi.log
# StandardError=append:/var/log/fastapi.log

[Install]
WantedBy=multi-user.target
"""

def deploy_one(host: str, cluster: str):
    print(f"üöÄ {host} ({cluster}) as {SSH_USER}")

    apt_prep = "sudo rm -f /etc/apt/apt.conf.d/50command-not-found || true"
    fix_lists = "sudo rm -rf /var/lib/apt/lists/* && sudo mkdir -p /var/lib/apt/lists/partial && sudo apt-get clean"
    apt_update = (
        f"{apt_prep}; "
        "sudo apt-get update -y "
        "|| (" + fix_lists + " && sudo apt-get update -y) "
        "|| true"
    )

    setup_cmds = [
        apt_update,
        # add psmisc for fuser
        "sudo DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends python3 python3-pip curl psmisc "
        "|| (" + fix_lists + " && sudo apt-get update -y && "
        "sudo DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends python3 python3-pip curl psmisc)",
        "mkdir -p ~/app && rm -rf ~/app/*",
    ]
    for c in setup_cmds:
        r = ssh(host, c)
        if r.returncode != 0:
            print(r.stdout); raise RuntimeError(f"[{host}] Failed: {c}")

    r = scp_dir(host, str(APP_SRC))
    if r.returncode != 0:
        print(r.stdout); raise RuntimeError(f"[{host}] SCP failed")

    for c in [
        "python3 -m pip install --upgrade pip",
        "python3 -m pip install fastapi 'uvicorn[standard]'",
    ]:
        r = ssh(host, c)
        if r.returncode != 0:
            print(r.stdout); raise RuntimeError(f"[{host}] Failed: {c}")

    unit_text = SERVICE_TPL.format(cluster=cluster)
    unit_b64  = base64.b64encode(unit_text.encode("utf-8")).decode("ascii")

    write_unit = (
        f"echo '{unit_b64}' | base64 -d | sudo tee {SERVICE_PATH} >/dev/null && "
        "sudo systemctl daemon-reload && "
        "sudo systemctl enable --now fastapi && "
        "sudo systemctl restart fastapi || true"
    )
    r = ssh(host, write_unit)
    if r.returncode != 0:
        print(r.stdout); raise RuntimeError(f"[{host}] Failed to install/start systemd unit")

    ready_cmd = (
        f"for i in $(seq 1 60); do "
        f"  code=$(curl -s -o /dev/null -w %{{http_code}} http://127.0.0.1:8000/{cluster}); "
        f"  [ \"$code\" = 200 ] && echo READY && exit 0; "
        f"  sleep 1; "
        f"done; echo NOT_READY;"
        f"sudo systemctl --no-pager --full status fastapi || true; "
        f"journalctl -u fastapi -n 120 --no-pager || true; exit 1"
    )
    r = ssh(host, ready_cmd)
    print(r.stdout, end="")
    if r.returncode != 0:
        raise RuntimeError(f"[{host}] App did not become ready")

def _task(inst):
    ip = inst.get("public_ip")
    cluster = inst.get("cluster", "")
    if not ip or not cluster:
        return (ip or "unknown", False, "missing ip/cluster")
    if ONLY_CLUSTER and cluster != ONLY_CLUSTER:
        return (ip, True, "skipped by ONLY_CLUSTER")
    try:
        deploy_one(ip, cluster)
        return (ip, True, "")
    except Exception as e:
        return (ip, False, str(e))

def main():
    todo = [i for i in instances if i.get("public_ip") and i.get("cluster")]
    print(f"Deploying to {len(todo)} host(s) with {MAX_WORKERS} worker(s)‚Ä¶")

    failures = 0
    with cf.ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
        futures = [ex.submit(_task, i) for i in todo]
        for fut in cf.as_completed(futures):
            ip, ok, err = fut.result()
            if ok:
                print(f"‚úÖ {ip} done")
            else:
                failures += 1
                print(f"‚ùå {ip} {err}")

    if failures:
        sys.exit(f"{failures} host(s) failed")
    print("‚úÖ Deployment complete!")

if __name__ == "__main__":
    main()
```

---

### How to use it

* Default parallelism = `min(8, cpu_count)`; override with env var:

  ```bash
  DEPLOY_CONCURRENCY=8 python scripts/deploy_fastapi.py
  ```
* Optionally limit to one cluster to iterate faster during dev:

  ```bash
  ONLY_CLUSTER=cluster2 DEPLOY_CONCURRENCY=8 python scripts/deploy_fastapi.py
  ```

---

### Expected speedup

* Provisioning wait: single waiter call vs. N sequential `wait_until_running` ‚Üí **\~N√ó faster wait phase**.
* Deployment: N hosts in parallel; bottlenecks (apt/pip) now run concurrently. A safe starting point is **6‚Äì8 workers** to avoid API/SSH saturation.

If you want, we can also add a **tiny wheel cache** (or pin `uvicorn` without `[standard]`) to shave a bit more install time, but the parallelism above already gives the big win.
